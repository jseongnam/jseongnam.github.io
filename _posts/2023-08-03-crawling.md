---
layout: post
title: "crawling"
---

# crawling

python으로 웹 크롤링을 해서 데이터를 가져와야 할 일이 생겼다. 그래서 정보를 찾아봤다.

## selenium

요즘 대세는 selenium 인 것 같다. selenium 내부에 webdriver.chrome.options, webdriver.common.by, webdriver.support.ui, webdriver.support 등등 정말 많다.

## 소스코드

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
driver = webdriver.Chrome(service= Service(ChromeDriverManager().install()))

driver = webdriver.Chrome()

jpg_xpath_pattern = '//*[@id="ulContentStr"]/li[{}]/a/div[2]/img' # 1~21
more_xpath_pattern = '//*[@id="aMore"]' # 더보기 버튼
wait = WebDriverWait(driver, 10);
# main page 경로는 다 똑같음
# jpg1 xpath : //*[@id="ulContentStr"]/li[1]/a/div[2]/img
# jpg2 xpath : //*[@id="ulContentStr"]/li[2]/a/div[2]/img
# jpg3 xpath : //*[@id="ulContentStr"]/li[3]/a/div[2]/img
# jpg4 xpath : //*[@id="ulContentStr"]/li[4]/a/div[2]/img
# jpg21 xpath : //*[@id="ulContentStr"]/li[21]/a/div[2]/img
# jpg22 xpath : //*[@id="ulContentStr"]/li[22]/a/div[2]/img
# jpg42 xpath : //*[@id="ulContentStr"]/li[42]/a/div[2]/img
# 더보기 xpath : //*[@id="aMore"]
# 1~21 보고
# 더보기 누르고
# 22~42 보고
# 더보기 누르고
# 43~63 보기
# error 발생할떄까지 무한반복
# title : //*[@id="PostView"]/div[1]/div[1]/div[2]/h3
# date : //*[@id="PostView"]/div[1]/div[1]/div[3]/div[1]/div[2]
# content : //*[@id="ContentView"]
driver.get('https://www.brainmedia.co.kr/MediaContent/MediaContentList.aspx?MenuCd=BrainScience');
reply = 0;
while 1 :
 for i in range(1+reply*21,22+reply*21) :
     try :
         driver.get('https://www.brainmedia.co.kr/MediaContent/MediaContentList.aspx?MenuCd=BrainScience');
         for _ in range(reply) :
             more_Link = wait.until(EC.element_to_be_clickable((By.XPATH, more_xpath_pattern)))
             more_Link.click();
         img = jpg_xpath_pattern.format(i);
         img_Link = wait.until(EC.element_to_be_clickable((By.XPATH, img)))
         img_Link.click();
         title = wait.until(EC.presence_of_element_located((By.XPATH, "//*[@id='PostView']/div[1]/div[1]/div[2]/h3")))
         date = wait.until(EC.presence_of_element_located((By.XPATH, "//*[@id='PostView']/div[1]/div[1]/div[3]/div[1]/div[2]")))
         content = wait.until(EC.presence_of_element_located((By.XPATH, "//*[@id='ContentView']")))
         # print(title.text); # 파일 제목, 파일 최상단
         # print(date.text);  # 파일 최상단
         # print(content.text); # 그 밑 데이터
         file = open(title.text+".txt",'w')
         file.write("title:"+ title.text+'\n')
         file.write("publish_date:"+date.text+'\n')
         file.write(content.text)
         file.close()
     except Exception as e :
         continue
 reply += 1;
driver.quit()
```
코드를 요약하자면, driver.get으로 항상 똑같은 경로에서 시작해서, 안으로 들어가서 crawling해준 뒤, 얻은 데이터를 바탕으로 문서를 만들어준 것이다. 어찌보면 간단해 보이지만, 거의 모든 것이 새로 쓰는 모듈이라 시간이 꽤 오래 걸렷다. 
코드 작성 시간 : 2시간 
총 소요시간 : 4시간 + 알파 
