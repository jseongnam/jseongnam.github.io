---
layout: post
title: "crawling"
---

# crawling

python으로 웹 크롤링을 해서 데이터를 가져와야 할 일이 생겼다. 그래서 정보를 찾아봤다.

## selenium

파이썬으로 웹 크롤링을 하는데 두가지 모듈이 있다. bueatuiful soup와 selenium이 그것이다. 그런데 bs은 동적 웹페이지에서는 안먹히고, 요새는 대부분이 동적 웹페이지이다. 떄문에 요즘 대세는 selenium 인 것 같다. selenium 내부에 webdriver.chrome.options, webdriver.common.by, webdriver.support.ui, webdriver.support 등등 정말 많다.

## 소스코드

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
driver = webdriver.Chrome(service= Service(ChromeDriverManager().install()))

driver = webdriver.Chrome()

jpg_xpath_pattern = '//*[@id="ulContentStr"]/li[{}]/a/div[2]/img' # 1~21
more_xpath_pattern = '//*[@id="aMore"]' # 더보기 버튼
wait = WebDriverWait(driver, 10);
# main page 경로는 다 똑같음
# jpg1 xpath : //*[@id="ulContentStr"]/li[1]/a/div[2]/img
# jpg2 xpath : //*[@id="ulContentStr"]/li[2]/a/div[2]/img
# jpg3 xpath : //*[@id="ulContentStr"]/li[3]/a/div[2]/img
# jpg4 xpath : //*[@id="ulContentStr"]/li[4]/a/div[2]/img
# jpg21 xpath : //*[@id="ulContentStr"]/li[21]/a/div[2]/img
# jpg22 xpath : //*[@id="ulContentStr"]/li[22]/a/div[2]/img
# jpg42 xpath : //*[@id="ulContentStr"]/li[42]/a/div[2]/img
# 더보기 xpath : //*[@id="aMore"]
# 1~21 보고
# 더보기 누르고
# 22~42 보고
# 더보기 누르고
# 43~63 보기
# error 발생할떄까지 무한반복
# title : //*[@id="PostView"]/div[1]/div[1]/div[2]/h3
# date : //*[@id="PostView"]/div[1]/div[1]/div[3]/div[1]/div[2]
# content : //*[@id="ContentView"]
driver.get('https://www.brainmedia.co.kr/MediaContent/MediaContentList.aspx?MenuCd=BrainScience');
reply = 0;
while 1 :
 for i in range(1+reply*21,22+reply*21) :
     try :
         driver.get('https://www.brainmedia.co.kr/MediaContent/MediaContentList.aspx?MenuCd=BrainScience');
         for _ in range(reply) :
             more_Link = wait.until(EC.element_to_be_clickable((By.XPATH, more_xpath_pattern)))
             more_Link.click();
         img = jpg_xpath_pattern.format(i);
         img_Link = wait.until(EC.element_to_be_clickable((By.XPATH, img)))
         img_Link.click();
         title = wait.until(EC.presence_of_element_located((By.XPATH, "//*[@id='PostView']/div[1]/div[1]/div[2]/h3")))
         date = wait.until(EC.presence_of_element_located((By.XPATH, "//*[@id='PostView']/div[1]/div[1]/div[3]/div[1]/div[2]")))
         content = wait.until(EC.presence_of_element_located((By.XPATH, "//*[@id='ContentView']")))
         # print(title.text); # 파일 제목, 파일 최상단
         # print(date.text);  # 파일 최상단
         # print(content.text); # 그 밑 데이터
         file = open(title.text+".txt",'w')
         file.write("title:"+ title.text+'\n')
         file.write("publish_date:"+date.text+'\n')
         file.write(content.text)
         file.close()
     except Exception as e :
         continue
 reply += 1;
driver.quit()
```
코드를 요약하자면, driver.get으로 항상 똑같은 경로에서 시작해서, 안으로 들어가서 crawling해준 뒤, 얻은 데이터를 바탕으로 문서를 만들어준 것이다. 어찌보면 간단해 보이지만, 거의 모든 것이 새로 쓰는 모듈이라 시간이 꽤 오래 걸렷다. 
# 코드 작성 시간 : 2시간 
# 총 소요시간 : 4시간 + 알파 
시간을 줄이고자 여러가지 정보를 찾아봤고, multiprocessing을 하면 시간을 획기적으로 줄일 수 있다는 것을 알았다. multiprocessing을 하기 위해서는, 실제로 사용하는 함수를 따로 둬야 한다. 그리고 그걸 import 하는 식으로 하여야 한다. 
# crawling.py
```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
def crawlingByDate(date):
    driver = webdriver.Chrome()
    wait = WebDriverWait(driver,5)
    url_format = 'https://news.kbs.co.kr/news/list.do?ctcd=0007&ref=pSiteMap#{}&1'
    url = url_format.format(date)
    driver.get(url)
    index = 0
    image_xpath_format1 = '//*[@id="thumbnailNewsList"]/li[{}]/a/span[1]/span/span/img'
    image_xpath_format2 = '//*[@id="thumbnailNewsList"]/li[{}]/a/span[1]/span[1]/span/img'
    while 1:
        index += 1
        try:
            image_xpath = image_xpath_format1.format(index)
            img = wait.until(EC.element_to_be_clickable((By.XPATH, image_xpath)))
            print("img:",img)

            img.click()
            title = driver.find_element(By.XPATH, '//*[@id="content"]/div/div[1]/div[1]/div[1]/h5')#//*[@id="content"]/div/div[1]/div[1]/div[1]/h5
            date = driver.find_element(By.XPATH, '//*[@id="content"]/div/div[1]/div[1]/div[1]/span/em[1]')#//*[@id="content"]/div/div[1]/div[1]/div[1]/span/em[1]
            content = driver.find_element(By.XPATH, '//*[@id="cont_newstext"]') #//*[@id="cont_newstext"]
            file = open(title.text + ".txt", "w")
            file.write("title:"+title.text+"\n")
            file.write("publish_date:"+date.text + "\n\n")
            file.write(content.text)
            file.close()
        except:
            try:
                image_xpath = image_xpath_format2.format(index)
                img = wait.until(EC.element_to_be_clickable((By.XPATH, image_xpath)))
                img.click()
                title = driver.find_element(By.XPATH, '//*[@id="content"]/div/div[1]/div[1]/div[1]/h5')#//*[@id="content"]/div/div[1]/div[1]/div[1]/h5
                date = driver.find_element(By.XPATH, '//*[@id="content"]/div/div[1]/div[1]/div[1]/span/em[1]')#//*[@id="content"]/div/div[1]/div[1]/div[1]/span/em[1]
                content = driver.find_element(By.XPATH, '//*[@id="cont_newstext"]') #//*[@id="cont_newstext"]
                file = open(title.text + ".txt", "w")
                file.write("title:"+title.text+"\n")
                file.write("publish_date:"+date.text + "\n\n")
                file.write(content.text)
                file.close()
            except :
                driver.quit()
                return  
```

# main.py
```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from multiprocessing import Pool, freeze_support
from crawling import crawlingByDate
from datetime import date, timedelta, datetime
monthArray = ['01','02','03','04','05','06','07','08','09','10','11','12']
dateArray = ['01','02','03','04','05','06','07','08','09','10','11','12','13'
             ,'14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30'
            ]
if __name__ == '__main__':
    freeze_support()

    for i in monthArray:
        fullArray = []

        if i == '02':
            pool_size = 28
            days = dateArray[:28]
        elif i in ['01', '03', '05', '07', '08', '10', '12']:
            pool_size = 31
            days = dateArray[:31]
        elif i in ['04', '06', '09', '11']:
            pool_size = 30
            days = dateArray[:30]

        for day in days:
            fullArray.append('2021' + i + day)
        with Pool(processes=8) as pool:
            pool.map(crawlingByDate, fullArray)
            print('month:',i)
```

이렇게 코드를 짜니 실제로 걸리는 시간이 1/10으로 줄어들었다. 